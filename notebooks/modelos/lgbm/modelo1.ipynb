{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0546eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PWC\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pred_lgbm as pred\n",
    "import funciones_lgbm as f_lgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1674fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ridge_optuna(data, test_size=0.2, random_state=42, n_trials=100, timeout=300):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: DataFrame completo con todas las columnas (incluyendo Salary)\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        random_state: Semilla aleatoria\n",
    "        n_trials: N√∫mero m√°ximo de pruebas de Optuna\n",
    "        timeout: Tiempo l√≠mite en segundos para la optimizaci√≥n\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Entrenando modelo Ridge Regression con Optuna + Features Estad√≠sticos\")\n",
    "    print(f\"   Trials: {n_trials}, Timeout: {timeout}s\")\n",
    "    \n",
    "    # Importaciones necesarias\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import optuna\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    # Silenciar warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    # ============= PASO 1: PREPARAR GROUPING INFO =============\n",
    "    print(\"üìä Paso 1: Preparando informaci√≥n de agrupaci√≥n...\")\n",
    "    data_with_groups, grouping_info = f_lgbm.create_and_save_grouping_info(data)\n",
    "    all_job_cats, all_seniority_cats = f_lgbm.get_all_categories(data_with_groups)\n",
    "    \n",
    "    print(f\"   ‚úÖ Grupos creados: Age_group, Exp_group\")\n",
    "    print(f\"   ‚úÖ Job categories: {len(all_job_cats)}\")\n",
    "    print(f\"   ‚úÖ Seniority levels: {len(all_seniority_cats)}\")\n",
    "    \n",
    "    # ============= PASO 2: SEPARAR TARGET Y FEATURES =============\n",
    "    \n",
    "    print(\"üîÑ Paso 2: Separando target y features...\")\n",
    "    \n",
    "    X_data = data_with_groups.drop('Salary', axis=1)  # Variables disponibles en producci√≥n\n",
    "    y = data_with_groups['Salary']  # Target\n",
    "    \n",
    "    print(f\"   üìä Datos originales: {X_data.shape}\")\n",
    "    print(f\"   üéØ Target: {len(y)} registros\")\n",
    "    \n",
    "    # ============= PASO 3: SPLIT PRINCIPAL TRAIN/TEST =============\n",
    "    print(\"‚úÇÔ∏è  Paso 3: Split principal train/test...\")\n",
    "    X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "        X_data, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"   üìà Train: {X_train_base.shape[0]} registros\")\n",
    "    print(f\"   üìâ Test:  {X_test_base.shape[0]} registros\")\n",
    "    \n",
    "    # ============= PASO 4: CREAR FEATURES CON ESTAD√çSTICAS =============\n",
    "    print(\"üîß Paso 4: Creando features con estad√≠sticas...\")\n",
    "    \n",
    "    # Crear features en TRAIN (calcula estad√≠sticas)\n",
    "    X_train, feature_names, stats_dict = f_lgbm.create_features_with_stats(\n",
    "        X_train_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=None,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    # Aplicar features a TEST (usa estad√≠sticas de train)\n",
    "    X_test, _ = f_lgbm.create_features_with_stats(\n",
    "        X_test_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=stats_dict,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Features totales: {X_train.shape[1]}\")\n",
    "    print(f\"   ‚úÖ Train: {X_train.shape}\")\n",
    "    print(f\"   ‚úÖ Test:  {X_test.shape}\")\n",
    "    \n",
    "    # ============= PASO 5: NORMALIZACI√ìN DE FEATURES =============\n",
    "    print(\"üîÑ Paso 5: Normalizando features (importante para Ridge)...\")\n",
    "    \n",
    "    # Inicializar scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Ajustar scaler en train y transformar\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"   ‚úÖ Features normalizadas\")\n",
    "    print(f\"   ‚úÖ Train scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"   ‚úÖ Test scaled: {X_test_scaled.shape}\")\n",
    "    \n",
    "    # ============= PASO 6: SPLIT PARA VALIDACI√ìN DE OPTUNA =============\n",
    "    print(\"üîÑ Paso 6: Split para validaci√≥n de Optuna...\")\n",
    "    X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"   üéØ Train opt: {X_train_opt.shape}\")\n",
    "    print(f\"   üîç Validation: {X_val_opt.shape}\")\n",
    "    \n",
    "    # ============= PASO 7: FUNCI√ìN OBJETIVO PARA OPTUNA =============\n",
    "    def objective(trial):\n",
    "        \"\"\"Funci√≥n objetivo para Optuna con Ridge Regression\"\"\"\n",
    "        \n",
    "        # Hiperpar√°metros a optimizar para Ridge\n",
    "        params = {\n",
    "            'alpha': trial.suggest_float('alpha', 0.001, 1000.0, log=True),\n",
    "            'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "            'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 10000),\n",
    "            'tol': trial.suggest_float('tol', 1e-6, 1e-2, log=True),\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        \n",
    "        # Crear y entrenar modelo\n",
    "        model = Ridge(**params)\n",
    "        \n",
    "        try:\n",
    "            # Entrenar modelo\n",
    "            model.fit(X_train_opt, y_train_opt)\n",
    "            \n",
    "            # Predecir en conjunto de validaci√≥n\n",
    "            y_pred = model.predict(X_val_opt)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_opt, y_pred))\n",
    "            \n",
    "            return rmse\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Si hay error, devolver un valor alto\n",
    "            return float('inf')\n",
    "    \n",
    "    # ============= PASO 8: OPTIMIZACI√ìN CON OPTUNA =============\n",
    "    print(\"üéØ Paso 8: Optimizando hiperpar√°metros con Optuna...\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    \n",
    "    # Optimizar\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Optimizaci√≥n completada: {len(study.trials)} trials realizados\")\n",
    "    print(f\"   üèÜ Mejor RMSE de validaci√≥n: ${study.best_value:,.2f}\")\n",
    "    \n",
    "    # ============= PASO 9: MODELO FINAL =============\n",
    "    print(\"üèÜ Paso 9: Entrenando modelo final...\")\n",
    "    \n",
    "    # Obtener mejores par√°metros\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params['random_state'] = random_state\n",
    "    \n",
    "    print(\"   üìã Mejores hiperpar√°metros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        if param != 'random_state':\n",
    "            print(f\"      {param}: {value}\")\n",
    "    \n",
    "    # Entrenar modelo final con mejores par√°metros\n",
    "    final_model = Ridge()\n",
    "    \n",
    "    try:\n",
    "        # Entrenar en todo el conjunto de entrenamiento escalado\n",
    "        final_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # ============= PASO 10: EVALUACI√ìN FINAL =============\n",
    "        print(\"üìä Paso 10: Evaluaci√≥n final...\")\n",
    "        \n",
    "        # Predicciones finales\n",
    "        y_pred = final_model.predict(X_test_scaled)\n",
    "        \n",
    "        # M√©tricas en conjunto de prueba\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Cross-validation con el modelo optimizado\n",
    "        print(\"   üîÑ Realizando validaci√≥n cruzada final...\")\n",
    "        cv_model = Ridge(**best_params)\n",
    "        cv_scores = cross_val_score(\n",
    "            cv_model, X_train_scaled, y_train, cv=5, \n",
    "            scoring='neg_mean_squared_error', n_jobs=-1\n",
    "        )\n",
    "        cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "        cv_std = np.sqrt(cv_scores.std())\n",
    "        \n",
    "        # ============= PASO 11: PREPARAR RESULTADOS =============\n",
    "        print(\"üì¶ Paso 11: Preparando resultados finales...\")\n",
    "        \n",
    "        # Calcular importancia de features usando coeficientes\n",
    "        feature_importance = np.abs(final_model.coef_)\n",
    "        \n",
    "        # Resultados del modelo\n",
    "        model_metrics = {\n",
    "            'model': final_model,\n",
    "            'scaler': scaler,  # Importante guardar el scaler\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mae': mae,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'cv_std': cv_std,\n",
    "            'predictions': y_pred,\n",
    "            'feature_importances': feature_importance,\n",
    "            'coefficients': final_model.coef_,\n",
    "            'intercept': final_model.intercept_,\n",
    "            'best_params': best_params,\n",
    "            'optuna_study': study\n",
    "        }\n",
    "        \n",
    "        # Resultado completo para compatibilidad\n",
    "        final_results = {\n",
    "            'model_results': {'Ridge_Optuna': model_metrics},\n",
    "            'best_model_name': 'Ridge_Optuna',\n",
    "            'best_model': final_model,\n",
    "            'scaler': scaler,  # Importante para predictions futuras\n",
    "            'feature_names': feature_names,\n",
    "            'job_categories': all_job_cats,\n",
    "            'seniority_categories': all_seniority_cats,\n",
    "            'stats_dict': stats_dict,\n",
    "            'grouping_info': grouping_info,\n",
    "            'X_test': X_test_scaled,  # Datos escalados\n",
    "            'y_test': y_test,\n",
    "            'X_train': X_train_scaled,  # Datos escalados\n",
    "            'y_train': y_train,\n",
    "            'optimization_study': study\n",
    "        }\n",
    "        \n",
    "        # ============= MOSTRAR RESULTADOS =============\n",
    "        print(f\"\\nüéâ RESULTADOS FINALES:\")\n",
    "        print(f\"   RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"   R¬≤: {r2:.3f}\")\n",
    "        print(f\"   MAE: ${mae:,.2f}\")\n",
    "        print(f\"   CV RMSE: ${cv_rmse:,.2f} (¬±{cv_std:,.2f})\")\n",
    "        print(f\"   Features totales: {len(feature_names)}\")\n",
    "        print(f\"   Alpha √≥ptimo: {best_params['alpha']:.6f}\")\n",
    "        print(f\"   Mejora vs RMSE de validaci√≥n: {((study.best_value - rmse) / study.best_value * 100):+.2f}%\")\n",
    "        print(f\"   Coeficientes no-cero: {np.sum(np.abs(final_model.coef_) > 1e-10)}\")\n",
    "        \n",
    "        return final_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error entrenando modelo final: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6aedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ridge_optuna_cv(data, test_size=0.2, random_state=42, n_trials=100, timeout=300):\n",
    "    \"\"\"\n",
    "    Entrena Ridge Regression con y sin Optuna, usando validaci√≥n cruzada interna para Optuna.\n",
    "    Compara ambos en el mismo test set y devuelve resultados completos.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüöÄ Entrenando Ridge Regression (default vs Optuna)\")\n",
    "    print(f\"   Trials: {n_trials}, Timeout: {timeout}s\")\n",
    "\n",
    "    # Importaciones\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import optuna\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    # ===================== Preprocesamiento =====================\n",
    "    print(\"üîÑ Preprocesando datos...\")\n",
    "\n",
    "    data_with_groups, grouping_info = f_lgbm.create_and_save_grouping_info(data)\n",
    "    all_job_cats, all_seniority_cats = f_lgbm.get_all_categories(data_with_groups)\n",
    "\n",
    "    X_data = data_with_groups.drop('Salary', axis=1)\n",
    "    y = data_with_groups['Salary']\n",
    "\n",
    "    X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "        X_data, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train, feature_names, stats_dict = f_lgbm.create_features_with_stats(\n",
    "        X_train_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=None,\n",
    "        is_training=True\n",
    "    )\n",
    "\n",
    "    X_test, _ = f_lgbm.create_features_with_stats(\n",
    "        X_test_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=stats_dict,\n",
    "        is_training=False\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # ===================== Ridge default =====================\n",
    "    print(\"\\n‚úÖ Entrenando Ridge (default)...\")\n",
    "    ridge_default = Ridge()\n",
    "    ridge_default.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred_default = ridge_default.predict(X_test_scaled)\n",
    "    rmse_default = np.sqrt(mean_squared_error(y_test, y_pred_default))\n",
    "    r2_default = r2_score(y_test, y_pred_default)\n",
    "    mae_default = mean_absolute_error(y_test, y_pred_default)\n",
    "\n",
    "    # ===================== Optuna objective con CV =====================\n",
    "    print(\"\\nüéØ Optimizando hiperpar√°metros con Optuna (CV interno)...\")\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'alpha': trial.suggest_float('alpha', 0.001, 1000.0, log=True),\n",
    "            'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "            'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 10000),\n",
    "            'tol': trial.suggest_float('tol', 1e-6, 1e-2, log=True),\n",
    "            'random_state': random_state\n",
    "        }\n",
    "\n",
    "        model = Ridge(**params)\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        rmse = np.sqrt(-scores.mean())\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params['random_state'] = random_state\n",
    "\n",
    "    print(\"   üìã Mejores hiperpar√°metros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        if param != 'random_state':\n",
    "            print(f\"      {param}: {value}\")\n",
    "\n",
    "    # ===================== Ridge con best_params =====================\n",
    "    print(\"\\n‚úÖ Entrenando Ridge (Optuna)...\")\n",
    "    ridge_optuna = Ridge(**best_params)\n",
    "    ridge_optuna.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred_optuna = ridge_optuna.predict(X_test_scaled)\n",
    "    rmse_optuna = np.sqrt(mean_squared_error(y_test, y_pred_optuna))\n",
    "    r2_optuna = r2_score(y_test, y_pred_optuna)\n",
    "    mae_optuna = mean_absolute_error(y_test, y_pred_optuna)\n",
    "\n",
    "    # ===================== Comparaci√≥n final =====================\n",
    "    print(\"\\nüèÅ Comparaci√≥n final en Test set:\")\n",
    "    print(f\"   ‚û° Ridge Default:  RMSE = ${rmse_default:,.2f} | R¬≤ = {r2_default:.3f} | MAE = ${mae_default:,.2f}\")\n",
    "    print(f\"   ‚û° Ridge Optuna:   RMSE = ${rmse_optuna:,.2f} | R¬≤ = {r2_optuna:.3f} | MAE = ${mae_optuna:,.2f}\")\n",
    "\n",
    "    if rmse_optuna < rmse_default:\n",
    "        print(\"‚úÖ üèÜ Optuna mejora el modelo final.\")\n",
    "        best_model = ridge_optuna\n",
    "        best_model_name = \"Ridge_Optuna\"\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è üîé El modelo default es mejor. Se recomienda mantenerlo.\")\n",
    "        best_model = ridge_default\n",
    "        best_model_name = \"Ridge_Default\"\n",
    "\n",
    "    # ===================== Resultados finales =====================\n",
    "    final_results = {\n",
    "        'ridge_default': {\n",
    "            'model': ridge_default,\n",
    "            'rmse': rmse_default,\n",
    "            'r2': r2_default,\n",
    "            'mae': mae_default,\n",
    "            'predictions': y_pred_default,\n",
    "        },\n",
    "        'ridge_optuna': {\n",
    "            'model': ridge_optuna,\n",
    "            'rmse': rmse_optuna,\n",
    "            'r2': r2_optuna,\n",
    "            'mae': mae_optuna,\n",
    "            'predictions': y_pred_optuna,\n",
    "            'best_params': best_params,\n",
    "            'optuna_study': study,\n",
    "        },\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'job_categories': all_job_cats,\n",
    "        'seniority_categories': all_seniority_cats,\n",
    "        'stats_dict': stats_dict,\n",
    "        'grouping_info': grouping_info,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_test': y_test,\n",
    "        'X_train': X_train_scaled,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e67217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_random_forest_optuna_cv(data, test_size=0.2, random_state=42, n_trials=100, timeout=300):\n",
    "    \"\"\"\n",
    "    Entrena Random Forest con y sin Optuna, usando validaci√≥n cruzada interna para Optuna.\n",
    "    Compara ambos en el mismo test set y devuelve resultados completos.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüöÄ Entrenando Random Forest (default vs Optuna)\")\n",
    "    print(f\"   Trials: {n_trials}, Timeout: {timeout}s\")\n",
    "\n",
    "    # Importaciones\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import optuna\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    # ===================== Preprocesamiento =====================\n",
    "    print(\"üîÑ Preprocesando datos...\")\n",
    "\n",
    "    data_with_groups, grouping_info = f_lgbm.create_and_save_grouping_info(data)\n",
    "    all_job_cats, all_seniority_cats = f_lgbm.get_all_categories(data_with_groups)\n",
    "\n",
    "    X_data = data_with_groups.drop('Salary', axis=1)\n",
    "    y = data_with_groups['Salary']\n",
    "\n",
    "    X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "        X_data, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train, feature_names, stats_dict = f_lgbm.create_features_with_stats(\n",
    "        X_train_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=None,\n",
    "        is_training=True\n",
    "    )\n",
    "\n",
    "    X_test, _ = f_lgbm.create_features_with_stats(\n",
    "        X_test_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=stats_dict,\n",
    "        is_training=False\n",
    "    )\n",
    "\n",
    "    # Random Forest no necesita escalado, pero lo mantenemos para consistencia\n",
    "    print(f\"‚úÖ Features preparadas: {X_train.shape}\")\n",
    "\n",
    "    # ===================== Random Forest default =====================\n",
    "    print(\"\\n‚úÖ Entrenando Random Forest (default)...\")\n",
    "    rf_default = RandomForestRegressor(random_state=random_state)\n",
    "    rf_default.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_default = rf_default.predict(X_test)\n",
    "    rmse_default = np.sqrt(mean_squared_error(y_test, y_pred_default))\n",
    "    r2_default = r2_score(y_test, y_pred_default)\n",
    "    mae_default = mean_absolute_error(y_test, y_pred_default)\n",
    "\n",
    "    # ===================== Optuna objective con CV =====================\n",
    "    print(\"\\nüéØ Optimizando hiperpar√°metros con Optuna (CV interno)...\")\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'max_samples': trial.suggest_float('max_samples', 0.5, 1.0) if trial.suggest_categorical('bootstrap_check', [True, False]) else None,\n",
    "            'random_state': random_state,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Limpiar par√°metros None o incompatibles\n",
    "        if not params['bootstrap']:\n",
    "            params.pop('max_samples', None)\n",
    "        if params['max_features'] == 'auto':\n",
    "            params['max_features'] = 'sqrt'  # 'auto' est√° deprecated\n",
    "        \n",
    "        model = RandomForestRegressor(**params)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        rmse = np.sqrt(-scores.mean())\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params['random_state'] = random_state\n",
    "    best_params['n_jobs'] = -1\n",
    "    \n",
    "    # Limpiar par√°metros de Optuna que no son del modelo\n",
    "    best_params.pop('bootstrap_check', None)\n",
    "    \n",
    "    # Limpiar par√°metros incompatibles\n",
    "    if not best_params.get('bootstrap', True):\n",
    "        best_params.pop('max_samples', None)\n",
    "    if best_params.get('max_features') == 'auto':\n",
    "        best_params['max_features'] = 'sqrt'\n",
    "\n",
    "    print(\"   üìã Mejores hiperpar√°metros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        if param not in ['random_state', 'n_jobs']:\n",
    "            print(f\"      {param}: {value}\")\n",
    "\n",
    "    # ===================== Random Forest con best_params =====================\n",
    "    print(\"\\n‚úÖ Entrenando Random Forest (Optuna)...\")\n",
    "    rf_optuna = RandomForestRegressor(**best_params)\n",
    "    rf_optuna.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_optuna = rf_optuna.predict(X_test)\n",
    "    rmse_optuna = np.sqrt(mean_squared_error(y_test, y_pred_optuna))\n",
    "    r2_optuna = r2_score(y_test, y_pred_optuna)\n",
    "    mae_optuna = mean_absolute_error(y_test, y_pred_optuna)\n",
    "\n",
    "    # ===================== Comparaci√≥n final =====================\n",
    "    print(\"\\nüèÅ Comparaci√≥n final en Test set:\")\n",
    "    print(f\"   ‚û° Random Forest Default:  RMSE = ${rmse_default:,.2f} | R¬≤ = {r2_default:.3f} | MAE = ${mae_default:,.2f}\")\n",
    "    print(f\"   ‚û° Random Forest Optuna:   RMSE = ${rmse_optuna:,.2f} | R¬≤ = {r2_optuna:.3f} | MAE = ${mae_optuna:,.2f}\")\n",
    "\n",
    "    if rmse_optuna < rmse_default:\n",
    "        print(\"‚úÖ üèÜ Optuna mejora el modelo final.\")\n",
    "        best_model = rf_optuna\n",
    "        best_model_name = \"RandomForest_Optuna\"\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è üîé El modelo default es mejor. Se recomienda mantenerlo.\")\n",
    "        best_model = rf_default\n",
    "        best_model_name = \"RandomForest_Default\"\n",
    "\n",
    "    # ===================== Feature Importance =====================\n",
    "    print(\"\\nüîç Top 10 Features m√°s importantes:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "    # ===================== Resultados finales =====================\n",
    "    final_results = {\n",
    "        'rf_default': {\n",
    "            'model': rf_default,\n",
    "            'rmse': rmse_default,\n",
    "            'r2': r2_default,\n",
    "            'mae': mae_default,\n",
    "            'predictions': y_pred_default,\n",
    "        },\n",
    "        'rf_optuna': {\n",
    "            'model': rf_optuna,\n",
    "            'rmse': rmse_optuna,\n",
    "            'r2': r2_optuna,\n",
    "            'mae': mae_optuna,\n",
    "            'predictions': y_pred_optuna,\n",
    "            'best_params': best_params,\n",
    "            'optuna_study': study,\n",
    "        },\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_model,\n",
    "        'scaler': None,  # Random Forest no necesita scaler\n",
    "        'feature_names': feature_names,\n",
    "        'feature_importance': feature_importance,\n",
    "        'job_categories': all_job_cats,\n",
    "        'seniority_categories': all_seniority_cats,\n",
    "        'stats_dict': stats_dict,\n",
    "        'grouping_info': grouping_info,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Funci√≥n adicional para guardar el modelo Random Forest\n",
    "def save_random_forest_model(results, filename=\"../../../modelos/random_forest_optuna.pkl\"):\n",
    "    \"\"\"\n",
    "    Guarda el mejor modelo Random Forest con formato compatible\n",
    "    \"\"\"\n",
    "    print(\"üíæ Guardando modelo Random Forest...\")\n",
    "    \n",
    "    best_model = results['best_model']\n",
    "    best_name = results['best_model_name']\n",
    "    \n",
    "    # Calcular m√©tricas del mejor modelo\n",
    "    if 'optuna' in best_name.lower():\n",
    "        metrics = {\n",
    "            'rmse': results['rf_optuna']['rmse'],\n",
    "            'r2': results['rf_optuna']['r2'],\n",
    "            'mae': results['rf_optuna']['mae']\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'rmse': results['rf_default']['rmse'],\n",
    "            'r2': results['rf_default']['r2'],\n",
    "            'mae': results['rf_default']['mae']\n",
    "        }\n",
    "    \n",
    "    # Crear paquete compatible\n",
    "    model_package = {\n",
    "        'model': best_model,\n",
    "        'model_name': best_name,\n",
    "        'feature_names': results['feature_names'],\n",
    "        'job_categories': results['job_categories'],\n",
    "        'seniority_categories': results['seniority_categories'],\n",
    "        'stats_dict': results['stats_dict'],\n",
    "        'grouping_info': results['grouping_info'],\n",
    "        'total_features': len(results['feature_names']),\n",
    "        'training_data_shape': results['X_train'].shape,\n",
    "        'has_statistical_features': True,\n",
    "        'is_ensemble': False,\n",
    "        'feature_importance': results['feature_importance'],\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(model_package, filename)\n",
    "        print(f\"‚úÖ Modelo guardado en: {filename}\")\n",
    "        print(f\"   üéØ Modelo: {best_name}\")\n",
    "        print(f\"   üìä RMSE: ${metrics['rmse']:,.2f}\")\n",
    "        print(f\"   üìà R¬≤: {metrics['r2']:.3f}\")\n",
    "        print(f\"   üìâ MAE: ${metrics['mae']:,.2f}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error guardando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "# Funci√≥n de ejemplo de uso\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Ejemplo de c√≥mo usar la funci√≥n\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Cargar datos\n",
    "    data = pd.read_csv('../../../dataC/imputado.csv').dropna()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    results = train_model_random_forest_optuna_cv(\n",
    "        data=data,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_trials=50,  # Reducir para pruebas r√°pidas\n",
    "        timeout=300\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo\n",
    "    save_random_forest_model(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5c981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO PIPELINE\n",
      "\n",
      "üöÄ Entrenando Random Forest (default vs Optuna)\n",
      "   Trials: 100, Timeout: 300s\n",
      "üîÑ Preprocesando datos...\n",
      "üìä Creando grupos y guardando informaci√≥n de rangos...\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (TRAIN)...\n",
      "   üîÑ Calculando estad√≠sticas en TRAIN (solo variables de producci√≥n)...\n",
      "   ‚úÖ Estad√≠sticas calculadas para 7 grupos\n",
      "   ‚úÖ Creadas 32 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 99\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 32\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (PREDICT)...\n",
      "   üì• Usando estad√≠sticas pre-calculadas de TRAIN...\n",
      "   ‚úÖ Creadas 32 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 99\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 32\n",
      "‚úÖ Features preparadas: (295, 99)\n",
      "\n",
      "‚úÖ Entrenando Random Forest (default)...\n",
      "\n",
      "üéØ Optimizando hiperpar√°metros con Optuna (CV interno)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 75. Best value: 11928.3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:43<00:00,  2.29it/s, 43.59/300 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìã Mejores hiperpar√°metros encontrados:\n",
      "      n_estimators: 436\n",
      "      max_depth: 30\n",
      "      min_samples_split: 3\n",
      "      min_samples_leaf: 1\n",
      "      max_features: log2\n",
      "      bootstrap: False\n",
      "\n",
      "‚úÖ Entrenando Random Forest (Optuna)...\n",
      "\n",
      "üèÅ Comparaci√≥n final en Test set:\n",
      "   ‚û° Random Forest Default:  RMSE = $17,587.88 | R¬≤ = 0.877 | MAE = $9,698.65\n",
      "   ‚û° Random Forest Optuna:   RMSE = $15,361.44 | R¬≤ = 0.906 | MAE = $8,881.49\n",
      "‚úÖ üèÜ Optuna mejora el modelo final.\n",
      "\n",
      "üîç Top 10 Features m√°s importantes:\n",
      "   age_vs_gender_mean: 0.0553\n",
      "   age_exp_interaction: 0.0473\n",
      "   age_rank_in_gender: 0.0437\n",
      "   age: 0.0431\n",
      "   start_year: 0.0429\n",
      "   exp_percentile_global: 0.0407\n",
      "   age_zscore_global: 0.0395\n",
      "   experience_age_ratio: 0.0394\n",
      "   age_edu: 0.0354\n",
      "   age_zscore_vs_gender: 0.0353\n",
      "\n",
      "üéâ AN√ÅLISIS COMPLETADO!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def pipeline_fe():\n",
    "    print(\"üöÄ INICIANDO PIPELINE\")\n",
    "\n",
    "    # 1 . Cargar datos\n",
    "    data = pd.read_csv('../../../dataC/imputado.csv')\n",
    "    data[\"Description\"] = data[\"Description\"].fillna(\"\")\n",
    "    \n",
    "    data = data.dropna()\n",
    "    # Se mejora levemente los errores realizando esta imputaci√≥n y dejando los otros nulos de los otros features.\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model_results =train_model_random_forest_optuna_cv(data)\n",
    "\n",
    "    # Analizar optimizaci√≥n\n",
    "    #best_params, best_value = f_lgbm.analyze_optuna_optimization(model_results['optimization_study'])\n",
    "\n",
    "    # Visualizar proceso\n",
    "    #plot_optuna_optimization(model_results['optimization_study'])\n",
    "\n",
    "    # 5. Analizar importancia\n",
    "    #feature_importance = fun.analyze_feature_importance(X,feature_names,model)\n",
    "\n",
    "    # 6. Analizar predicciones\n",
    "    #predictions_analysis = f.analyze_predictions(model_results)\n",
    "\n",
    "    # 7. Comparar modelos\n",
    "    #fun.create_comparison_chart(model_results)\n",
    "\n",
    "    print(\"\\nüéâ AN√ÅLISIS COMPLETADO!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Normalizar nombre\n",
    "    best_name = model_results['best_model_name']\n",
    "    \"\"\"\n",
    "    # Asegurar consistencia\n",
    "    if best_name.lower() == 'ridge_optuna':\n",
    "        best_name_key = 'ridge_optuna'\n",
    "    elif best_name.lower() == 'ridge_default':\n",
    "        best_name_key = 'ridge_default'\n",
    "    else:\n",
    "        raise ValueError(f\"Nombre de modelo inesperado: {best_name}\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Obtener el dict\n",
    "    #best_result = model_results[best_name_key]\n",
    "\n",
    "    #print(f\"\\nüèÜ RESUMEN FINAL:\")\n",
    "    #print(f\"   Mejor modelo: {best_name}\")\n",
    "    #print(f\"   RMSE: ${best_result['rmse']:,.2f}\")\n",
    "    #print(f\"   R¬≤: {best_result['r2']:.3f}\")\n",
    "    #print(f\"   MAE: ${best_result['mae']:,.2f}\")\n",
    "        \n",
    "    \n",
    "\n",
    "     \n",
    "    return model_results,data\n",
    "    # ,model,X,feature_importance\n",
    "    \n",
    "results,data=pipeline_fe()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e443bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_random_forest_results(new_data, rf_results):\n",
    "    \"\"\"\n",
    "    Funci√≥n especial para predecir con los resultados de Random Forest\n",
    "    Compatible con la estructura devuelta por train_model_random_forest_optuna_cv()\n",
    "    \"\"\"\n",
    "    print(\"üå≤ Predicci√≥n con Random Forest (funci√≥n especial)...\")\n",
    "    \n",
    "    # Extraer el mejor modelo y componentes necesarios\n",
    "    best_model = rf_results.get('best_model')\n",
    "    best_model_name = rf_results.get('best_model_name', 'Unknown')\n",
    "    feature_names = rf_results.get('feature_names', [])\n",
    "    job_categories = rf_results.get('job_categories', [])\n",
    "    seniority_categories = rf_results.get('seniority_categories', [])\n",
    "    stats_dict = rf_results.get('stats_dict', {})\n",
    "    grouping_info = rf_results.get('grouping_info', {})\n",
    "    scaler = rf_results.get('scaler')  # None para Random Forest\n",
    "    \n",
    "    print(f\"   ü§ñ Modelo: {best_model_name}\")\n",
    "    print(f\"   üî¢ Features disponibles: {len(feature_names)}\")\n",
    "    \n",
    "    # Verificar que tenemos todo lo necesario\n",
    "    if best_model is None:\n",
    "        print(\"‚ùå Error: No hay modelo en rf_results\")\n",
    "        return None\n",
    "    \n",
    "    if not hasattr(best_model, 'predict'):\n",
    "        print(\"‚ùå Error: El modelo no tiene m√©todo predict\")\n",
    "        return None\n",
    "    \n",
    "    # Crear grupos si no existen\n",
    "    input_data_copy = new_data.copy()\n",
    "    if 'Exp_group' not in input_data_copy.columns or 'Age_group' not in input_data_copy.columns:\n",
    "        for idx, row in input_data_copy.iterrows():\n",
    "            # Usar la funci√≥n calculate_groups de tu librer√≠a\n",
    "            exp_group, age_group = pred.calculate_groups(\n",
    "                age=row['Age'], \n",
    "                years_of_experience=row['Years_of_Experience'], \n",
    "                grouping_info=grouping_info\n",
    "            )\n",
    "            input_data_copy.at[idx, 'Exp_group'] = exp_group\n",
    "            input_data_copy.at[idx, 'Age_group'] = age_group\n",
    "    \n",
    "    # Crear features usando tu funci√≥n de la librer√≠a\n",
    "    try:\n",
    "        X_features, created_feature_names = f_lgbm.create_features_with_stats(\n",
    "            input_data_copy,\n",
    "            all_job_categories=job_categories,\n",
    "            all_seniority_levels=seniority_categories,\n",
    "            stats_dict=stats_dict,\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Features creadas: {X_features.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando features: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Verificar dimensiones\n",
    "    if len(created_feature_names) != len(feature_names):\n",
    "        print(f\"   ‚ö†Ô∏è Ajustando dimensiones: {len(created_feature_names)} ‚Üí {len(feature_names)}\")\n",
    "        \n",
    "        # Alinear con features del modelo entrenado\n",
    "        X_aligned = pd.DataFrame(0, index=X_features.index, columns=feature_names)\n",
    "        \n",
    "        # Llenar con los valores disponibles\n",
    "        for col in X_features.columns:\n",
    "            if col in X_aligned.columns:\n",
    "                X_aligned[col] = X_features[col]\n",
    "        \n",
    "        X_features = X_aligned\n",
    "        print(f\"   ‚úÖ Dimensiones alineadas: {X_features.shape}\")\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    try:\n",
    "        # Random Forest no necesita escalado\n",
    "        prediction = best_model.predict(X_features)[0]\n",
    "        \n",
    "        print(f\"   üí∞ Predicci√≥n Random Forest: ${prediction:,.2f}\")\n",
    "        print(f\"   ‚úÖ Predicci√≥n exitosa\")\n",
    "        \n",
    "        # Mostrar m√©tricas del modelo si est√°n disponibles\n",
    "        if 'optuna' in best_model_name.lower():\n",
    "            metrics = rf_results.get('rf_optuna', {})\n",
    "        else:\n",
    "            metrics = rf_results.get('rf_default', {})\n",
    "        \n",
    "        if metrics:\n",
    "            print(f\"   üìä M√©tricas del modelo:\")\n",
    "            print(f\"      RMSE: ${metrics.get('rmse', 0):,.2f}\")\n",
    "            print(f\"      R¬≤: {metrics.get('r2', 0):.3f}\")\n",
    "            print(f\"      MAE: ${metrics.get('mae', 0):,.2f}\")\n",
    "        \n",
    "        return prediction\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en predicci√≥n: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_random_forest_prediction(rf_results):\n",
    "    \"\"\"\n",
    "    Test espec√≠fico para Random Forest\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing predicci√≥n Random Forest...\")\n",
    "    \n",
    "    # Crear registro de prueba\n",
    "    test_record = pd.DataFrame({\n",
    "        'Age': [60],\n",
    "        'Gender': ['Male'],\n",
    "        'Education_Level': [\"PhD\"],\n",
    "        'Job_Title': ['CEO'],\n",
    "        'Years_of_Experience': [24],\n",
    "        'Description': ['I work with machine learning models and data analysis']\n",
    "    })\n",
    "    \n",
    "    print(f\"üìã Registro de prueba:\")\n",
    "    print(f\"   üë§ Edad: {test_record['Age'][0]} a√±os\")\n",
    "    print(f\"   üë® G√©nero: {test_record['Gender'][0]}\")\n",
    "    print(f\"   üéì Educaci√≥n: {test_record['Education_Level'][0]}\")\n",
    "    print(f\"   üíº Puesto: {test_record['Job_Title'][0]}\")\n",
    "    print(f\"   üìà Experiencia: {test_record['Years_of_Experience'][0]} a√±os\")\n",
    "    \n",
    "    try:\n",
    "        prediction = predict_random_forest_results(test_record, rf_results)\n",
    "        \n",
    "        if prediction is not None:\n",
    "            print(f\"‚úÖ Test exitoso: Predicci√≥n = ${prediction:,.2f}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Test fall√≥: La predicci√≥n devolvi√≥ None\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test fall√≥: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e23799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Predicci√≥n con Random Forest (funci√≥n especial)...\n",
      "   ü§ñ Modelo: RandomForest_Optuna\n",
      "   üî¢ Features disponibles: 99\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (PREDICT)...\n",
      "   üì• Usando estad√≠sticas pre-calculadas de TRAIN...\n",
      "   ‚úÖ Creadas 26 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 93\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 26\n",
      "   ‚úÖ Features creadas: (1, 93)\n",
      "   ‚ö†Ô∏è Ajustando dimensiones: 93 ‚Üí 99\n",
      "   ‚úÖ Dimensiones alineadas: (1, 99)\n",
      "   üí∞ Predicci√≥n Random Forest: $189,403.67\n",
      "   ‚úÖ Predicci√≥n exitosa\n",
      "   üìä M√©tricas del modelo:\n",
      "      RMSE: $15,361.44\n",
      "      R¬≤: 0.906\n",
      "      MAE: $8,881.49\n",
      "üß™ Testing predicci√≥n Random Forest...\n",
      "üìã Registro de prueba:\n",
      "   üë§ Edad: 60 a√±os\n",
      "   üë® G√©nero: Male\n",
      "   üéì Educaci√≥n: PhD\n",
      "   üíº Puesto: CEO\n",
      "   üìà Experiencia: 24 a√±os\n",
      "üå≤ Predicci√≥n con Random Forest (funci√≥n especial)...\n",
      "   ü§ñ Modelo: RandomForest_Optuna\n",
      "   üî¢ Features disponibles: 99\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (PREDICT)...\n",
      "   üì• Usando estad√≠sticas pre-calculadas de TRAIN...\n",
      "   ‚úÖ Creadas 26 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 93\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 26\n",
      "   ‚úÖ Features creadas: (1, 93)\n",
      "   ‚ö†Ô∏è Ajustando dimensiones: 93 ‚Üí 99\n",
      "   ‚úÖ Dimensiones alineadas: (1, 99)\n",
      "   üí∞ Predicci√≥n Random Forest: $189,403.67\n",
      "   ‚úÖ Predicci√≥n exitosa\n",
      "   üìä M√©tricas del modelo:\n",
      "      RMSE: $15,361.44\n",
      "      R¬≤: 0.906\n",
      "      MAE: $8,881.49\n",
      "‚úÖ Test exitoso: Predicci√≥n = $189,403.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Despu√©s de entrenar tu modelo Random Forest\n",
    "rf_results = results\n",
    "# Para hacer predicciones individuales\n",
    "test_record = pd.DataFrame({\n",
    "    'Age': [60],\n",
    "    'Gender': ['Male'],\n",
    "    'Education_Level': [\"PhD\"],\n",
    "    'Job_Title': ['CEO'],\n",
    "    'Years_of_Experience': [24],\n",
    "    'Description': ['I work with machine learning models']\n",
    "})\n",
    "\n",
    "prediction = predict_random_forest_results(test_record, rf_results)\n",
    "\n",
    "# Para hacer tests\n",
    "test_random_forest_prediction(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3ee0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_random_forest_ensemble(data, n_forests=10, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un ensemble de m√∫ltiples Random Forest con diferentes configuraciones\n",
    "    \"\"\"\n",
    "    print(f\"üå≤ Creando ENSEMBLE de {n_forests} Random Forest...\")\n",
    "    \n",
    "    # Importaciones necesarias\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import numpy as np\n",
    "    \n",
    "    # Preparar datos igual que antes\n",
    "    data_with_groups, grouping_info = f_lgbm.create_and_save_grouping_info(data)\n",
    "    all_job_cats, all_seniority_cats = f_lgbm.get_all_categories(data_with_groups)\n",
    "    \n",
    "    X_data = data_with_groups.drop('Salary', axis=1)\n",
    "    y = data_with_groups['Salary']\n",
    "    \n",
    "    X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "        X_data, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_train, feature_names, stats_dict = f_lgbm.create_features_with_stats(\n",
    "        X_train_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=None,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    X_test, _ = f_lgbm.create_features_with_stats(\n",
    "        X_test_base,\n",
    "        all_job_categories=all_job_cats,\n",
    "        all_seniority_levels=all_seniority_cats,\n",
    "        stats_dict=stats_dict,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Datos preparados: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONFIGURACIONES DIFERENTES PARA CADA RANDOM FOREST\n",
    "    # ============================================================================\n",
    "    \n",
    "    rf_configs = [\n",
    "        {\n",
    "            'name': 'RF_Deep',\n",
    "            'params': {\n",
    "                'n_estimators': 200,\n",
    "                'max_depth': 25,\n",
    "                'min_samples_split': 2,\n",
    "                'min_samples_leaf': 1,\n",
    "                'max_features': 'sqrt',\n",
    "                'bootstrap': True,\n",
    "                'random_state': random_state + 1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'RF_Wide',\n",
    "            'params': {\n",
    "                'n_estimators': 150,\n",
    "                'max_depth': 15,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': 'log2',\n",
    "                'bootstrap': True,\n",
    "                'random_state': random_state + 2\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'RF_Conservative',\n",
    "            'params': {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 10,\n",
    "                'min_samples_split': 10,\n",
    "                'min_samples_leaf': 4,\n",
    "                'max_features': 0.8,\n",
    "                'bootstrap': True,\n",
    "                'random_state': random_state + 3\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'RF_Aggressive',\n",
    "            'params': {\n",
    "                'n_estimators': 300,\n",
    "                'max_depth': 30,\n",
    "                'min_samples_split': 2,\n",
    "                'min_samples_leaf': 1,\n",
    "                'max_features': None,\n",
    "                'bootstrap': True,\n",
    "                'random_state': random_state + 4\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'RF_Balanced',\n",
    "            'params': {\n",
    "                'n_estimators': 150,\n",
    "                'max_depth': 20,\n",
    "                'min_samples_split': 4,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': 'sqrt',\n",
    "                'bootstrap': True,\n",
    "                'random_state': random_state + 5\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Usar solo el n√∫mero de bosques solicitado\n",
    "    rf_configs = rf_configs[:n_forests]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ENTRENAR M√öLTIPLES RANDOM FOREST\n",
    "    # ============================================================================\n",
    "    \n",
    "    rf_models = {}\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i, config in enumerate(rf_configs):\n",
    "        name = config['name']\n",
    "        params = config['params']\n",
    "        \n",
    "        print(f\"üöÄ Entrenando {name} ({i+1}/{n_forests})...\")\n",
    "        print(f\"   üìä Par√°metros: n_estimators={params['n_estimators']}, max_depth={params['max_depth']}\")\n",
    "        \n",
    "        try:\n",
    "            # Crear y entrenar Random Forest\n",
    "            rf_model = RandomForestRegressor(**params, n_jobs=-1)\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Hacer predicciones\n",
    "            y_pred = rf_model.predict(X_test)\n",
    "            \n",
    "            # Calcular m√©tricas\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            \n",
    "            rf_models[name] = {\n",
    "                'model': rf_model,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'mae': mae,\n",
    "                'predictions': y_pred,\n",
    "                'config': config\n",
    "            }\n",
    "            \n",
    "            all_predictions.append(y_pred)\n",
    "            \n",
    "            print(f\"   ‚úÖ RMSE: ${rmse:,.2f}, R¬≤: {r2:.3f}, MAE: ${mae:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error entrenando {name}: {e}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CREAR ENSEMBLE DE RANDOM FORESTS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nüéØ Creando ensemble de {len(rf_models)} Random Forest...\")\n",
    "    \n",
    "    if len(all_predictions) == 0:\n",
    "        print(\"‚ùå No se pudo entrenar ning√∫n Random Forest\")\n",
    "        return None\n",
    "    \n",
    "    # M√©todo 1: Promedio simple\n",
    "    ensemble_simple = np.mean(all_predictions, axis=0)\n",
    "    simple_rmse = np.sqrt(mean_squared_error(y_test, ensemble_simple))\n",
    "    simple_r2 = r2_score(y_test, ensemble_simple)\n",
    "    simple_mae = mean_absolute_error(y_test, ensemble_simple)\n",
    "    \n",
    "    # M√©todo 2: Promedio ponderado por R¬≤\n",
    "    valid_models = {name: res for name, res in rf_models.items() if res['r2'] > 0}\n",
    "    r2_scores = [res['r2'] for res in valid_models.values()]\n",
    "    weights = np.array(r2_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    valid_predictions = [res['predictions'] for res in valid_models.values()]\n",
    "    ensemble_weighted = np.average(valid_predictions, axis=0, weights=weights)\n",
    "    weighted_rmse = np.sqrt(mean_squared_error(y_test, ensemble_weighted))\n",
    "    weighted_r2 = r2_score(y_test, ensemble_weighted)\n",
    "    weighted_mae = mean_absolute_error(y_test, ensemble_weighted)\n",
    "    \n",
    "    # M√©todo 3: Mediana (m√°s robusto a outliers)\n",
    "    ensemble_median = np.median(all_predictions, axis=0)\n",
    "    median_rmse = np.sqrt(mean_squared_error(y_test, ensemble_median))\n",
    "    median_r2 = r2_score(y_test, ensemble_median)\n",
    "    median_mae = mean_absolute_error(y_test, ensemble_median)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # COMPARAR M√âTODOS DE ENSEMBLE\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nüìä Comparaci√≥n de m√©todos de ensemble:\")\n",
    "    print(f\"   üîπ Promedio Simple:    RMSE=${simple_rmse:,.2f}, R¬≤={simple_r2:.3f}, MAE=${simple_mae:,.2f}\")\n",
    "    print(f\"   üîπ Promedio Ponderado: RMSE=${weighted_rmse:,.2f}, R¬≤={weighted_r2:.3f}, MAE=${weighted_mae:,.2f}\")\n",
    "    print(f\"   üîπ Mediana:           RMSE=${median_rmse:,.2f}, R¬≤={median_r2:.3f}, MAE=${median_mae:,.2f}\")\n",
    "    \n",
    "    # Determinar el mejor m√©todo\n",
    "    ensemble_methods = {\n",
    "        'simple': {'rmse': simple_rmse, 'r2': simple_r2, 'mae': simple_mae, 'predictions': ensemble_simple},\n",
    "        'weighted': {'rmse': weighted_rmse, 'r2': weighted_r2, 'mae': weighted_mae, 'predictions': ensemble_weighted},\n",
    "        'median': {'rmse': median_rmse, 'r2': median_r2, 'mae': median_mae, 'predictions': ensemble_median}\n",
    "    }\n",
    "    \n",
    "    best_method = min(ensemble_methods.keys(), key=lambda x: ensemble_methods[x]['rmse'])\n",
    "    best_metrics = ensemble_methods[best_method]\n",
    "    \n",
    "    print(f\"\\nüèÜ Mejor m√©todo: {best_method.upper()}\")\n",
    "    print(f\"   üìä RMSE: ${best_metrics['rmse']:,.2f}\")\n",
    "    print(f\"   üìà R¬≤: {best_metrics['r2']:.3f}\")\n",
    "    print(f\"   üìâ MAE: ${best_metrics['mae']:,.2f}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # FEATURE IMPORTANCE COMBINADA\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nüîç Calculando feature importance combinada...\")\n",
    "    \n",
    "    # Combinar feature importance de todos los modelos\n",
    "    all_importances = []\n",
    "    for name, result in rf_models.items():\n",
    "        importance = result['model'].feature_importances_\n",
    "        all_importances.append(importance)\n",
    "    \n",
    "    # Promedio de importancias\n",
    "    avg_importance = np.mean(all_importances, axis=0)\n",
    "    \n",
    "    # Crear DataFrame de feature importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': avg_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"üîç Top 10 Features m√°s importantes (promedio):\")\n",
    "    for i, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RESULTADOS FINALES\n",
    "    # ============================================================================\n",
    "    \n",
    "    ensemble_results = {\n",
    "        'individual_models': rf_models,\n",
    "        'ensemble_methods': ensemble_methods,\n",
    "        'best_method': best_method,\n",
    "        'best_predictions': best_metrics['predictions'],\n",
    "        'best_metrics': best_metrics,\n",
    "        'feature_importance': feature_importance_df,\n",
    "        'weights': weights if best_method == 'weighted' else None,\n",
    "        'feature_names': feature_names,\n",
    "        'job_categories': all_job_cats,\n",
    "        'seniority_categories': all_seniority_cats,\n",
    "        'stats_dict': stats_dict,\n",
    "        'grouping_info': grouping_info,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'ensemble_type': 'multiple_random_forest'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ensemble de Random Forest creado exitosamente!\")\n",
    "    print(f\"   üå≤ Bosques entrenados: {len(rf_models)}\")\n",
    "    print(f\"   üéØ Mejor m√©todo: {best_method}\")\n",
    "    print(f\"   üìä Mejora promedio: Eval√∫a vs modelos individuales\")\n",
    "    \n",
    "    return ensemble_results\n",
    "\n",
    "def predict_multiple_rf_ensemble(new_data, ensemble_results):\n",
    "    \"\"\"\n",
    "    Funci√≥n de predicci√≥n para ensemble de m√∫ltiples Random Forest\n",
    "    \"\"\"\n",
    "    print(\"üå≤ Predicci√≥n con ENSEMBLE de m√∫ltiples Random Forest...\")\n",
    "    \n",
    "    # Obtener componentes necesarios\n",
    "    individual_models = ensemble_results['individual_models']\n",
    "    best_method = ensemble_results['best_method']\n",
    "    weights = ensemble_results.get('weights')\n",
    "    expected_feature_names = ensemble_results['feature_names']\n",
    "    \n",
    "    print(f\"   üî¢ Features esperadas por los modelos: {len(expected_feature_names)}\")\n",
    "    \n",
    "    # Crear grupos si no existen\n",
    "    input_data_copy = new_data.copy()\n",
    "    if 'Exp_group' not in input_data_copy.columns or 'Age_group' not in input_data_copy.columns:\n",
    "        for idx, row in input_data_copy.iterrows():\n",
    "            exp_group, age_group = pred.calculate_groups(\n",
    "                age=row['Age'], \n",
    "                years_of_experience=row['Years_of_Experience'], \n",
    "                grouping_info=ensemble_results['grouping_info']\n",
    "            )\n",
    "            input_data_copy.at[idx, 'Exp_group'] = exp_group\n",
    "            input_data_copy.at[idx, 'Age_group'] = age_group\n",
    "    \n",
    "    # Crear features usando los mismos par√°metros del entrenamiento\n",
    "    X_features, created_feature_names = f_lgbm.create_features_with_stats(\n",
    "        input_data_copy,\n",
    "        all_job_categories=ensemble_results['job_categories'],\n",
    "        all_seniority_levels=ensemble_results['seniority_categories'],\n",
    "        stats_dict=ensemble_results['stats_dict'],\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    print(f\"   üî¢ Features creadas: {len(created_feature_names)}\")\n",
    "    print(f\"   üîç Primeras 5 features creadas: {created_feature_names[:5]}\")\n",
    "    print(f\"   üîç Primeras 5 features esperadas: {expected_feature_names[:5]}\")\n",
    "    \n",
    "    # ALINEAR FEATURES con las que esperan los modelos\n",
    "    if len(created_feature_names) != len(expected_feature_names) or created_feature_names != expected_feature_names:\n",
    "        print(f\"   ‚ö†Ô∏è Ajustando features: {len(created_feature_names)} ‚Üí {len(expected_feature_names)}\")\n",
    "        \n",
    "        # Crear DataFrame con todas las features esperadas, inicializadas en 0\n",
    "        X_aligned = pd.DataFrame(0, index=X_features.index, columns=expected_feature_names)\n",
    "        \n",
    "        # Llenar con los valores disponibles\n",
    "        for col in X_features.columns:\n",
    "            if col in X_aligned.columns:\n",
    "                X_aligned[col] = X_features[col]\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Feature '{col}' no est√° en el modelo entrenado\")\n",
    "        \n",
    "        # Verificar qu√© features faltan\n",
    "        missing_features = set(expected_feature_names) - set(created_feature_names)\n",
    "        if missing_features:\n",
    "            print(f\"   üîç Features faltantes (se rellenan con 0): {len(missing_features)}\")\n",
    "            print(f\"      Ejemplos: {list(missing_features)[:5]}\")\n",
    "        \n",
    "        X_features = X_aligned\n",
    "        print(f\"   ‚úÖ Features alineadas: {X_features.shape}\")\n",
    "    \n",
    "    # Hacer predicciones con cada Random Forest\n",
    "    predictions = []\n",
    "    model_names = []\n",
    "    \n",
    "    for name, result in individual_models.items():\n",
    "        try:\n",
    "            model = result['model']\n",
    "            \n",
    "            # Verificar que las columnas coincidan exactamente\n",
    "            if hasattr(model, 'feature_names_in_'):\n",
    "                model_features = list(model.feature_names_in_)\n",
    "                current_features = list(X_features.columns)\n",
    "                \n",
    "                if model_features != current_features:\n",
    "                    print(f\"   üîß Reordenando features para {name}...\")\n",
    "                    # Reordenar las columnas para que coincidan exactamente\n",
    "                    X_model = X_features.reindex(columns=model_features, fill_value=0)\n",
    "                else:\n",
    "                    X_model = X_features\n",
    "            else:\n",
    "                X_model = X_features\n",
    "            \n",
    "            model_prediction = model.predict(X_model)\n",
    "            \n",
    "            # Extraer el valor de predicci√≥n correctamente\n",
    "            if len(model_prediction) == 1:\n",
    "                pred_value = model_prediction[0]\n",
    "            else:\n",
    "                pred_value = model_prediction[0] if hasattr(model_prediction, '__len__') else model_prediction\n",
    "            \n",
    "            predictions.append(pred_value)\n",
    "            model_names.append(name)\n",
    "            print(f\"   ‚úÖ {name}: ${pred_value:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error con {name}: {e}\")\n",
    "            # Solo mostrar traceback si es necesario para debug\n",
    "            continue\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"‚ùå No se pudieron hacer predicciones\")\n",
    "        return None\n",
    "    \n",
    "    # Aplicar el mejor m√©todo de ensemble\n",
    "    if best_method == 'simple':\n",
    "        final_prediction = np.mean(predictions)\n",
    "    elif best_method == 'weighted' and weights is not None and len(weights) == len(predictions):\n",
    "        final_prediction = np.average(predictions, weights=weights[:len(predictions)])\n",
    "    elif best_method == 'median':\n",
    "        final_prediction = np.median(predictions)\n",
    "    else:\n",
    "        final_prediction = np.mean(predictions)  # Fallback\n",
    "    \n",
    "    print(f\"\\n   üéØ M√©todo usado: {best_method}\")\n",
    "    print(f\"   üí∞ Predicci√≥n final: ${final_prediction:,.2f}\")\n",
    "    print(f\"   ü§ñ Bosques usados: {len(predictions)}\")\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def test_multiple_rf_ensemble(ensemble_results):\n",
    "    \"\"\"\n",
    "    Test para ensemble de m√∫ltiples Random Forest\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing ensemble de m√∫ltiples Random Forest...\")\n",
    "    \n",
    "    # Crear registro de prueba\n",
    "    test_record = pd.DataFrame({\n",
    "        'Age': [60],\n",
    "        'Gender': ['Male'],\n",
    "        'Education_Level': [\"PhD\"],\n",
    "        'Job_Title': ['CEO'],\n",
    "        'Years_of_Experience': [24],\n",
    "        'Description': ['I work with machine learning models and data analysis']\n",
    "    })\n",
    "    \n",
    "    print(f\"üìã Registro de prueba:\")\n",
    "    print(f\"   üë§ Edad: {test_record['Age'][0]} a√±os\")\n",
    "    print(f\"   üë® G√©nero: {test_record['Gender'][0]}\")\n",
    "    print(f\"   üéì Educaci√≥n: {test_record['Education_Level'][0]}\")\n",
    "    print(f\"   üíº Puesto: {test_record['Job_Title'][0]}\")\n",
    "    print(f\"   üìà Experiencia: {test_record['Years_of_Experience'][0]} a√±os\")\n",
    "    \n",
    "    try:\n",
    "        prediction = predict_multiple_rf_ensemble(test_record, ensemble_results)\n",
    "        \n",
    "        if prediction is not None:\n",
    "            print(f\"‚úÖ Test exitoso: Predicci√≥n = ${prediction:,.2f}\")\n",
    "            \n",
    "            # Mostrar m√©tricas del ensemble\n",
    "            best_metrics = ensemble_results['best_metrics']\n",
    "            print(f\"   üìä M√©tricas del ensemble:\")\n",
    "            print(f\"      RMSE: ${best_metrics['rmse']:,.2f}\")\n",
    "            print(f\"      R¬≤: {best_metrics['r2']:.3f}\")\n",
    "            print(f\"      MAE: ${best_metrics['mae']:,.2f}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Test fall√≥: La predicci√≥n devolvi√≥ None\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test fall√≥: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejemplo de uso completo\n",
    "def example_multiple_rf_usage():\n",
    "    \"\"\"\n",
    "    Ejemplo completo de ensemble de m√∫ltiples Random Forest\n",
    "    \"\"\"\n",
    "    print(\"üå≤ Ejemplo: Ensemble de M√∫ltiples Random Forest\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data = pd.read_csv('../../../dataC/imputado.csv').dropna()\n",
    "    \n",
    "    # Crear ensemble de Random Forest\n",
    "    ensemble_results = create_multiple_random_forest_ensemble(\n",
    "        data=data,\n",
    "        n_forests=5,  # N√∫mero de bosques diferentes\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Test de predicci√≥n\n",
    "    test_multiple_rf_ensemble(ensemble_results)\n",
    "    \n",
    "    return ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a610b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Ejemplo: Ensemble de M√∫ltiples Random Forest\n",
      "üå≤ Creando ENSEMBLE de 5 Random Forest...\n",
      "üìä Creando grupos y guardando informaci√≥n de rangos...\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (TRAIN)...\n",
      "   üîÑ Calculando estad√≠sticas en TRAIN (solo variables de producci√≥n)...\n",
      "   ‚úÖ Estad√≠sticas calculadas para 7 grupos\n",
      "   ‚úÖ Creadas 32 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 99\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 32\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (PREDICT)...\n",
      "   üì• Usando estad√≠sticas pre-calculadas de TRAIN...\n",
      "   ‚úÖ Creadas 32 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 99\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 32\n",
      "‚úÖ Datos preparados: Train (292, 99), Test (74, 99)\n",
      "üöÄ Entrenando RF_Deep (1/5)...\n",
      "   üìä Par√°metros: n_estimators=200, max_depth=25\n",
      "   ‚úÖ RMSE: $15,898.37, R¬≤: 0.904, MAE: $9,330.24\n",
      "üöÄ Entrenando RF_Wide (2/5)...\n",
      "   üìä Par√°metros: n_estimators=150, max_depth=15\n",
      "   ‚úÖ RMSE: $15,835.37, R¬≤: 0.905, MAE: $9,217.38\n",
      "üöÄ Entrenando RF_Conservative (3/5)...\n",
      "   üìä Par√°metros: n_estimators=100, max_depth=10\n",
      "   ‚úÖ RMSE: $17,709.90, R¬≤: 0.881, MAE: $10,147.45\n",
      "üöÄ Entrenando RF_Aggressive (4/5)...\n",
      "   üìä Par√°metros: n_estimators=300, max_depth=30\n",
      "   ‚úÖ RMSE: $17,371.02, R¬≤: 0.886, MAE: $9,674.55\n",
      "üöÄ Entrenando RF_Balanced (5/5)...\n",
      "   üìä Par√°metros: n_estimators=150, max_depth=20\n",
      "   ‚úÖ RMSE: $16,155.07, R¬≤: 0.901, MAE: $9,393.55\n",
      "\n",
      "üéØ Creando ensemble de 5 Random Forest...\n",
      "\n",
      "üìä Comparaci√≥n de m√©todos de ensemble:\n",
      "   üîπ Promedio Simple:    RMSE=$16,309.71, R¬≤=0.899, MAE=$9,424.27\n",
      "   üîπ Promedio Ponderado: RMSE=$16,301.77, R¬≤=0.900, MAE=$9,420.63\n",
      "   üîπ Mediana:           RMSE=$16,159.92, R¬≤=0.901, MAE=$9,357.54\n",
      "\n",
      "üèÜ Mejor m√©todo: MEDIAN\n",
      "   üìä RMSE: $16,159.92\n",
      "   üìà R¬≤: 0.901\n",
      "   üìâ MAE: $9,357.54\n",
      "\n",
      "üîç Calculando feature importance combinada...\n",
      "üîç Top 10 Features m√°s importantes (promedio):\n",
      "   age_rank_in_gender: 0.1272\n",
      "   age_zscore_vs_gender: 0.0963\n",
      "   age_vs_gender_mean: 0.0864\n",
      "   exp_edu: 0.0820\n",
      "   age_exp_interaction: 0.0650\n",
      "   exp_rank_in_gender: 0.0599\n",
      "   age_zscore_global: 0.0285\n",
      "   age_edu: 0.0284\n",
      "   age: 0.0270\n",
      "   exp_zscore_vs_gender: 0.0264\n",
      "\n",
      "‚úÖ Ensemble de Random Forest creado exitosamente!\n",
      "   üå≤ Bosques entrenados: 5\n",
      "   üéØ Mejor m√©todo: median\n",
      "   üìä Mejora promedio: Eval√∫a vs modelos individuales\n"
     ]
    }
   ],
   "source": [
    "print(\"üå≤ Ejemplo: Ensemble de M√∫ltiples Random Forest\")\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv('../../../dataC/imputado.csv').dropna()\n",
    "\n",
    "# Crear ensemble de Random Forest\n",
    "ensemble_results = create_multiple_random_forest_ensemble(\n",
    "    data=data,\n",
    "    n_forests=5,  # N√∫mero de bosques diferentes\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Test de predicci√≥n\n",
    "#test_multiple_rf_ensemble(ensemble_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b441ec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing ensemble de m√∫ltiples Random Forest...\n",
      "üìã Registro de prueba:\n",
      "   üë§ Edad: 60 a√±os\n",
      "   üë® G√©nero: Male\n",
      "   üéì Educaci√≥n: PhD\n",
      "   üíº Puesto: CEO\n",
      "   üìà Experiencia: 24 a√±os\n",
      "üå≤ Predicci√≥n con ENSEMBLE de m√∫ltiples Random Forest...\n",
      "   üî¢ Features esperadas por los modelos: 99\n",
      "üîß Creando caracter√≠sticas completas para producci√≥n (originales + estad√≠sticos)...\n",
      "üîß Creando todas las caracter√≠sticas ...\n",
      "‚úÖ Creadas 67 caracter√≠sticas en total\n",
      "   - Variables num√©ricas b√°sicas: 3\n",
      "   - Variables de educaci√≥n: 3\n",
      "   - Variables de job category: 12\n",
      "   - Variables de seniority: 6\n",
      "   - Variables de texto: 4\n",
      "   - Ratios y scores: 5\n",
      "üìä Creando features estad√≠sticos para producci√≥n (PREDICT)...\n",
      "   üì• Usando estad√≠sticas pre-calculadas de TRAIN...\n",
      "   ‚úÖ Creadas 26 features estad√≠sticos para producci√≥n\n",
      "‚úÖ Features totales para producci√≥n: 93\n",
      "   - Originales: 67\n",
      "   - Estad√≠sticos: 26\n",
      "   üî¢ Features creadas: 93\n",
      "   üîç Primeras 5 features creadas: ['age', 'years_experience', 'age_experience_ratio', 'experience_squared', 'age_exp_interaction']\n",
      "   üîç Primeras 5 features esperadas: ['age', 'years_experience', 'age_experience_ratio', 'experience_squared', 'age_exp_interaction']\n",
      "   ‚ö†Ô∏è Ajustando features: 93 ‚Üí 99\n",
      "   üîç Features faltantes (se rellenan con 0): 6\n",
      "      Ejemplos: ['age_rank_in_gender', 'exp_rank_in_edu_gender', 'exp_rank_in_gender', 'exp_rank_in_edu_group', 'age_rank_in_edu_gender']\n",
      "   ‚úÖ Features alineadas: (1, 99)\n",
      "   ‚úÖ RF_Deep: $174,500.00\n",
      "   ‚úÖ RF_Wide: $167,656.14\n",
      "   ‚úÖ RF_Conservative: $129,529.34\n",
      "   ‚úÖ RF_Aggressive: $130,066.67\n",
      "   ‚úÖ RF_Balanced: $159,117.70\n",
      "\n",
      "   üéØ M√©todo usado: median\n",
      "   üí∞ Predicci√≥n final: $159,117.70\n",
      "   ü§ñ Bosques usados: 5\n",
      "‚úÖ Test exitoso: Predicci√≥n = $159,117.70\n",
      "   üìä M√©tricas del ensemble:\n",
      "      RMSE: $16,159.92\n",
      "      R¬≤: 0.901\n",
      "      MAE: $9,357.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_multiple_rf_ensemble(ensemble_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
